#### 代码课件之解释

这段代码用于加载并可视化一个数据集，下面我们对每一行代码进行详细解释，并提供一些背景信息，以便你更好地理解它们的用途和意义。

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
```

1. 库的导入：

numpy（简称np）：是一个非常流行的Python库，用于进行高效的数值计算，特别是在处理多维数组和矩阵时非常方便。
pandas（简称pd）：用于数据操作和分析。它特别擅长处理表格化的数据，如Excel表或SQL表格，并提供了强大的数据过滤、分组和聚合功能。
matplotlib.pyplot（简称plt）：是一个绘图库，常用于生成各种静态、动态和交互式可视化图表。
背景： 这些库是Python数据科学中的基础库，常用于数据加载、处理和可视化。

```python
data_path = './dataset/hour.csv'
```

2. 数据路径设置：

这里的data_path是用于指定CSV数据文件的路径。文件名是hour.csv，它保存在./dataset/目录下。
背景： CSV（Comma Separated Values）是一种常用的数据存储格式，采用纯文本保存数据；每一行表示一个数据记录，字段之间用逗号分隔。数据通常可以从数据库导出或电子表格存储为CSV格式。

```python
rides = pd.read_csv(data_path)
```

3. 数据加载：

使用pandas中的read_csv函数从指定路径加载CSV文件，并将其数据储存在rides变量中，rides是一个DataFrame（数据框）。
背景： DataFrame 是pandas中最核心的数据结构，类似于电子表格或SQL表格，它允许对数据进行各种操作，例如筛选、排序、删除、插入、统计聚合等。

```python
rides.head()
```

4. 数据预览：

head()方法用于查看DataFrame中的前5行数据。
背景： 初步检查加载的数据，以确保数据已经正确加载，并初步了解其结构，包括列名、数据类型以及前几行的内容。

```python
rides[:24*10].plot(x='dteday', y='cnt')
```

5. 数据可视化：

rides[:24*10]选取DataFrame的前240行数据（即10天的数据，假设每小时一行，共24小时），目的是减少数据量，方便快速绘图。
plot()方法用于生成图表，其中x='dteday'表示x轴为日期列，y='cnt'表示y轴为计数值列。
背景：

dteday：通常是日期时间信息列。
cnt：通常用于表示某个特定对象的计数，例如车流量或用户数量。
通过这种可视化，你可以快速了解某个时间范围内数据变化的趋势或模式。
总体背景： 这段代码可能用于分析某种基于时间的记录（如租车、自行车共享等），通过选择10天的数据来简化分析。绘制的图可以帮助我们识别每日活动的某种模式或变化趋势。这在时序数据分析中是一个常见的初步探索步骤。

___



接下来这段代码用于对数据进行预处理，特别是涉及类别变量的处理，并为机器学习模型准备数据。下面是对这段代码的详细解释：

```python
dummy_fields = ['season', 'weathersit', 'mnth', 'hr', 'weekday']
```

1. 定义类别字段：

dummy_fields 列表包含了需要进行独热编码（one-hot encoding）的类别字段，这些字段通常是分类变量，不适合直接输入到某些机器学习模型中。
列表中的字段：season（季节），weathersit（天气情况），mnth（月份），hr（小时），weekday（星期几）。
背景： 在数据处理中，类别变量需要转换成数值形式，而独热编码是常用的方法。它将每个类别变量转换为由0和1组成的向量，以便模型处理。

```python
for each in dummy_fields:
    dummies = pd.get_dummies(rides[each], prefix=each, drop_first=False)
    rides = pd.concat([rides, dummies], axis=1)
```

2. 进行独热编码：

通过循环对每个类别字段生成独热编码。
pd.get_dummies() 用于创建虚拟变量（dummy variables）或编码矩阵。prefix=each 为新列添加前缀，使用原列名，有助于后续识别。
drop_first=False 表示不删除第一个类别列，完整保留所有类别的独热编码。
编码后的数据使用 pd.concat() 方法合并到原始 rides 数据框中，axis=1 表示列合并。
背景： 独热编码生成的虚拟变量将类别信息转换为二进制（0或1）形式。例如，"season"是一个有4个独特值的变量，可能会变成四列（如season_1, season_2, season_3, season_4），每列表示某种季节。

```python
fields_to_drop = ['instant', 'dteday', 'season', 'weathersit', 
                  'weekday', 'atemp', 'mnth', 'workingday', 'hr']
```

3. 定义要移除的字段：

`fields_to_drop` 是一个包含了准备从数据集中删除的字段列表。
这些字段可能因为各类原因需要被移除，例如：已经被编码成虚拟变量、没有额外信息的时序信息，或者对分析不再重要。
背景： 数据集中的一些列（如时间戳等）在经过处理或编码后可能不再需要，移除这些能简化数据集，减少冗余，按需保留有价值的信息。

`data = rides.drop(fields_to_drop, axis=1)`

4. 删除不必要的字段：

使用 `rides.drop()` 从数据框中移除定义好的字段，`axis=1` 表明按列删除。
结果存储在名为 data 的新数据框中。
背景： drop() 是pandas数据处理的常用方法，删去不需要的列，为模型训练或者进一步分析准备更精简且有用的版本。

`data.head()`

5. 查看处理后的数据：

`data.head()` 用于查看 `data` 数据框的前5行以进行验证，从而确认已成功移除不必要的字段并生成独热编码。
背景： 此步骤是数据预处理的重要组成部分，通过显示数据框的一部分来检验操作效果，帮助确保在后续数据分析或模型训练之前数据格式和内容的正确性。

这些步骤的整体目的在于将原始输入数据转化为适宜用于机器学习模型的数据形式，尤其针对线性模型或其他无法直接处理分类数据的算法。这种处理能帮助模型更好地理解并利用数据特征进行预测或分类。



___



这段代码的目的是对数据集中的某些数值特征进行标准化处理。标准化有助于提高机器学习算法的性能和训练稳定性，特别是在特征的值域差别较大的情况下。这种预处理也是深度学习中常用的步骤。接下来，逐行解释这段代码：

```
quant_features = ['casual', 'registered', 'cnt', 'temp', 'hum', 'windspeed']
```

**1. 定义数值特征：**

- `quant_features` 是一个列表，包含了需要进行标准化处理的数值特征名。
- 这些特征通常以不同的量级或单位表示，因此需要标准化以便于模型利用。

**背景：** 这些特征可能代表数据集中重要的数值变量，例如：

- `casual` 和 `registered` 可能代表某种统计数量；
- `cnt` 是总量的计数；
- `temp`、`hum` 和 `windspeed` 可能分别代表温度、湿度和风速。

```
# Store scalings in a dictionary so we can convert back later
scaled_features = {}
```

**2. 初始化字典存储缩放参数：**

- `scaled_features` 字典用于存储每个特征对应的均值和标准差，以便日后逆标准化或进行其他分析。

**背景：** 记录下这些缩放参数是一个良好的实践，它可以帮助我们在对新数据进行预测时还原模型输出，或者评估真实意义上的误差。

```
for each in quant_features:
    mean, std = data[each].mean(), data[each].std()
    scaled_features[each] = [mean, std]
    data.loc[:, each] = (data[each] - mean)/std
```

**3. 执行标准化操作：**

- 通过遍历`quant_features`列表，对每个特征进行处理。
- `mean, std = data[each].mean(), data[each].std()`：计算特征的均值（mean）和标准差（std）。
- `scaled_features[each] = [mean, std]`：将这些统计量存储在`scaled_features`字典中，以提供后备支持。
- `data.loc[:, each] = (data[each] - mean)/std`：对每一特征进行标准化，将每个特征值减去其均值之后，除以标准差，结果是该特征具有均值为0和标准差为1的分布。

**背景：** 标准化过程中心化特征并将其尺度化为统一的标准差范围，改善算法表现，特别是涉及到距离度量的方法，如K近邻算法或神经网络。处理后，所有特征在模型中具有相同的权重。

```
print('done')
```

**4. 任务完成标识：**

- 打印信息表明标准化过程已完成。

**背景：** 使用简单的提示来标识完成点，有助于确认计算流程正常运行，尤其是在长时间处理中，提示信息是代码调试和验证的有用工具。

整体而言，这段代码进行的是数据预处理中的标准化步骤，这在准备输入数据时是至关重要的，特别当不同特征跨越不同范围的情况下，标准化使得它们在模型计算中表现得更加均衡且可比较。这样经过预处理的数据在输入模型后，可以帮助模型更快更可靠地收敛到良好的解决方案。

___

**这段代码**的目的是将数据集分成训练集和测试集，并分别提取特征和目标变量。这是机器学习和数据科学处理中非常常见的步骤。下面为你详细解释每一行代码：

```
# Save data for approximately the last 21 days 
test_data = data[-21*24:]
```

**1. 提取测试数据集：**

- `test_data = data[-21*24:]`：选择数据集中最后21天的数据作为测试集。假设数据集是按小时采样的（每小时一条记录），21天对应 `21*24` 条记录。
- 使用数据框在负索引范围内提取子集，意味着从数据集末尾开始选择记录。

**背景：** 在时间序列分析中，通常选取数据集中最新的数据作为测试集，以便评估模型在最近实际情况下运行的表现。

```
# Now remove the test data from the data set 
data = data[:-21*24]
```

**2. 获取训练数据集：**

- 从数据集中移除最后21天的数据，即 `data = data[:-21*24]` 。
- 这种做法是在拆分数据集，将旧数据用作训练，以便模型能够在模拟历史趋势和模式上进行学习。

**背景：** 训练集用于构建和调优模型，而测试集用于验证模型的泛化能力。确保测试数据完全独立于训练数据是评估模型的关键步骤。

```
# Separate the data into features and targets
target_fields = ['cnt', 'casual', 'registered']
features, targets = data.drop(target_fields, axis=1), data[target_fields]
```

**3. 分离特征与目标变量：**

- `target_fields` 列表中存储的是目标变量字段，这些是模型需要预测的值。
- `data.drop(target_fields, axis=1)`：从训练数据集中移除目标列，剩下的就是特征集（特征用于模型输入）。
- `data[target_fields]`：提取数据中定义的目标字段，组成的目标集是模型的输出。

**背景：** 特征和目标的拆分是准备机器学习模型的基本步骤。特征是模型输入，通常包括影响因子，而目标是模型希望预测的输出。

```
test_features, test_targets = test_data.drop(target_fields, axis=1), test_data[target_fields]
```

**4. 提取测试集中的特征和目标：**

- 类似于训练数据的处理，将`test_data`切分为`test_features`和`test_targets`。
- `test_features` 是测试集的输入，`test_targets` 是相应的真值输出，用于后续模型的评估。

**背景：** 使用测试集评估模型在未见过的数据上的性能是获得公平泛化误差估计的重要方法。

```
print('traing count={} test count={}'.format(len(features), len(test_features)))
```

**5. 打印数据集大小：**

- 使用格式化字符串输出训练集和测试集的样本数量。
- `len(features)` 和 `len(test_features)` 返回训练和测试特征集的样本数。

**背景：** 输出各数据集的大小以确保分割正确，并验证数据正确分配，利于后续调试和分析。

此过程可以帮助确保模型有足够的训练数据来学习模式，同时保留充分的测试数据以评估其在见过和未见数据之间的表现差异。数据集划分通常是数据科学项目计划中至关重要的一部分，有助于保证模型能够在现实环境中良好运作。

___

这段代码的目的是进一步将数据集划分为训练集和验证集，并为验证模型性能做好准备。划分验证集可以在训练过程中监控模型的性能，以避免过拟合。下面是对代码的详细解释：

```
# Hold out the last 60 days or so of the remaining data as a validation set
train_features, train_targets = features[:-60*24], targets[:-60*24]
val_features, val_targets = features[-60*24:], targets[-60*24:]
```

**1. 验证集划分：**

- 划分验证集

  ：从剩余数据（被原始数据分割出测试数据之后的部分）中提取最后60天的数据作为验证集。

  - `features[-60*24:]` 和 `targets[-60*24:]` 从 `features` 和 `targets` 中提取出最后60天的数据用作验证集。

- 划分训练集

  ：保留剩余的记录作为训练集。

  - `features[:-60*24]` 和 `targets[:-60*24]` 则是提取60天之前的数据作为训练集。

**背景：** 在数据科学和机器学习中，通常把数据集分成三个部分：

- **训练集**： 用于训练模型；
- **验证集**： 用于调参和选择模型，即用来评估并微调模型的性能，可以帮助决定何时停止训练以避免过拟合；
- **测试集**： 完全独立于模型训练，用于最终评估模型的性能。

通常，验证集是从训练数据中再分出一部分来，用于实时监控算法在未见数据上的性能表现。

```
print('traing count={} valid count={} test count={}'.format(len(train_features),len(val_features),len(test_features)))
```

**2. 打印数据集大小：**

- 通过`len(train_features)`，`len(val_features)`和`len(test_features)`来获得训练、验证和测试集的样本数量，并使用格式化字符串打印出来。

**背景：** 检查划分后的数据集的大小，以确保数据集划分合理且满足预期规格。打印出来的计数可以帮助我们在后续步骤中更容易跟踪问题，这也是调试和验证的一种有效途径。

整体过程确保数据集被合理划分，使模型可以在多个步骤中被适当评估和优化，即使测试集是完全独立的，验证集和训练集也可以在模型训练期间（通过适当选择超参数和监控模型性能）协助优化模型的表现。验证集的存在是模型选择和超参数调优的重要工具。

___



下面是这一段代码的详细解释。这段代码定义了一个两层的神经网络（含一个隐藏层），并实现了前向传播和反向传播的基本功能。代码的目的是训练一个用于回归任务的简单神经网络。

### 代码解析

```
class NeuralNetwork(object):
    def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate):
```

- **构造函数 `__init__`**：用于初始化神经网络的结构，包括输入层、隐藏层和输出层的节点数，以及学习率。

```
self.input_nodes = input_nodes
self.hidden_nodes = hidden_nodes
self.output_nodes = output_nodes
```

- 设置网络的输入、隐藏和输出节点的数量。

```
self.weights_input_to_hidden = np.random.normal(0.0, self.input_nodes**-0.5, 
                                   (self.input_nodes, self.hidden_nodes))
self.weights_hidden_to_output = np.random.normal(0.0, self.hidden_nodes**-0.5, 
                                   (self.hidden_nodes, self.output_nodes))
self.lr = learning_rate
```

- **权重初始化**：初始化输入层到隐藏层以及隐藏层到输出层的权重矩阵，使用正态分布。
- `np.random.normal(0.0, std, size)` 生成一个指定均值和标准差的正态分布随机数矩阵，`std` 通常设置为输入节点数的平方根的倒数，以防止早期层信号过强或过弱。

```
self.activation_function = lambda x: 1 / (1 + np.exp(-x))
```

- **Sigmoid 激活函数**：定义了一个标准的 sigmoid 函数，常用于隐藏层的激活。该函数将输入压缩到0和1之间。

```
def train(self, features, targets):
    n_records = features.shape[0]
    delta_weights_i_h = np.zeros(self.weights_input_to_hidden.shape)
    delta_weights_h_o = np.zeros(self.weights_hidden_to_output.shape)
```

- **训练方法 `train`**：用于通过一组特征（features）和目标（targets）进行模型训练。
- 初始化调整的权重变化值为零，`delta_weights_i_h` 和 `delta_weights_h_o` 分别是输入到隐藏和隐藏到输出的权重变化。

```
for X, y in zip(features, targets):
    hidden_inputs = np.dot(X, self.weights_input_to_hidden)
    hidden_outputs = self.activation_function(hidden_inputs)

    final_inputs = np.dot(hidden_outputs, self.weights_hidden_to_output)
    final_outputs = final_inputs
```

- 前向传播

  ：

  - **隐藏层计算**：计算输入到隐藏层节点的总信号，通过激活函数计算输出。
  - **输出层计算**：计算隐藏层输出信号到输出层的信号，该网络的输出层是线性的，因为它是回归任务，不需要进一步激活函数处理。

```
error = y - final_outputs
hidden_error = np.dot(error, self.weights_hidden_to_output.T)
output_error_term = error * 1.
hidden_error_term = hidden_error * hidden_outputs * (1 - hidden_outputs)
```

- 误差和梯度计算

  ：

  - **输出误差**：计算输出层误差为真实值与预测值的差异。
  - **隐藏层误差**：计算隐藏层的误差，这由输出层误差和隐藏层到输出层权重的转置一起影响。
  - **误差项**：输出层误差项是直接误差，因为无激活函数；隐藏层误差项需要考虑 Sigmoid 激活函数的导数。

```
delta_weights_i_h += hidden_error_term * X[:, None]
delta_weights_h_o += output_error_term * hidden_outputs[:, None]
```

- **累计权重变化**：累加对应层的权重变化。

```
self.weights_hidden_to_output += self.lr * delta_weights_h_o / n_records
self.weights_input_to_hidden += self.lr * delta_weights_i_h / n_records
```

- **更新权重**：根据平均误差梯度来更新权重。

```
def run(self, features):
    hidden_inputs = np.dot(features, self.weights_input_to_hidden)
    hidden_outputs = self.activation_function(hidden_inputs)

    final_inputs = np.dot(hidden_outputs, self.weights_hidden_to_output)
    final_outputs = final_inputs

    return final_outputs
```

- **运行网络 `run`**：进行推理，只包含前向传播部分，用于使用训练好的权重进行预测。

```
def MSE(y, Y):
    return np.mean((y-Y)**2)
```

- **均方误差函数**：一个简单的损失函数，用于衡量预测值与真实值之间的误差。

### 背景和流程

1. **前向传播**：通过网络层逐层传递输入数据，应用激活函数获得输出。
2. **反向传播**：通过计算误差和梯度调整每一层的权重，以便模型在下一次前向传播时能够做出更准确的预测。
3. **激活函数**：Sigmoid函数用于隐藏层，其导数影响反向传播中的梯度计算。
4. **超参和结构**：设定学习率和节点数，这些超参数对模型训练速度和准确度都有重大影响。

这一流程是神经网络学习的基础，通过迭代调整和优化网络的权重来逼近目标函数，这种方法在诸多回归和分类任务中广泛应用。

___

这段代码使用了 Python 的 `unittest` 框架来测试神经网络的正确性。它主要检查各种功能是否按预期工作，包括权重更新和网络的前向传播。下面是对这段代码的逐步解释。

### 代码解析

```
import unittest

inputs = np.array([[0.5, -0.2, 0.1]])
targets = np.array([[0.4]])
test_w_i_h = np.array([[0.1, -0.2],
                       [0.4, 0.5],
                       [-0.3, 0.2]])
test_w_h_o = np.array([[0.3],
                       [-0.1]])
```

- 定义测试数据

  :

  - `inputs` 和 `targets` 是一个输入特征矩阵和目标矩阵，分别用于训练和测试。
  - `test_w_i_h` 是用于输入层到隐藏层的测试权重矩阵。
  - `test_w_h_o` 是用于隐藏层到输出层的测试权重矩阵。

这些预设的权重和数据将用于验证网络训练后的状态，以确保实现符合预期。

```
class TestMethods(unittest.TestCase):
```

- **测试类 `TestMethods`**：这是一个基于 `unittest` 模块的测试类，它存储了多个用于验证网络功能的方法。

#### 数据加载测试方法

```
def test_data_path(self):
    # Test that file path to dataset has been unaltered
    self.assertTrue(data_path.lower() == 'bike-sharing-dataset/hour.csv')
```

- **`test_data_path`**：检查数据路径是否正确，它测试的是数据加载部分有没有被其他代码段意外修改。

```
def test_data_loaded(self):
    # Test that data frame loaded
    self.assertTrue(isinstance(rides, pd.DataFrame))
```

- **`test_data_loaded`**：确保数据已经以 `DataFrame` 格式成功加载进 `rides`，确认数据结构和格式是否正确。

#### 网络功能测试方法

```
def test_activation(self):
    network = NeuralNetwork(3, 2, 1, 0.5)
    # Test that the activation function is a sigmoid
    self.assertTrue(np.all(network.activation_function(0.5) == 1/(1+np.exp(-0.5))))
```

- **`test_activation`**：测试构造的神经网络的激活函数是否是 sigmoid 函数，通过应用测试值（如 0.5）进行对比，有助于确保激活函数的正确性。

```
def test_train(self):
    # Test that weights are updated correctly on training
    network = NeuralNetwork(3, 2, 1, 0.5)
    network.weights_input_to_hidden = test_w_i_h.copy()
    network.weights_hidden_to_output = test_w_h_o.copy()
    
    network.train(inputs, targets)
    self.assertTrue(np.allclose(network.weights_hidden_to_output, 
                                np.array([[ 0.37275328], 
                                          [-0.03172939]])))
    self.assertTrue(np.allclose(network.weights_input_to_hidden,
                                np.array([[ 0.10562014, -0.20185996], 
                                          [0.39775194, 0.50074398], 
                                          [-0.29887597, 0.19962801]])))
```

- `test_train`

  ：测试训练过程后的权重更新是否正确。

  - 初始化网络和设置初始权重。
  - 调用 `train()` 方法，以确保其返回后的权重是否与预期更新后的权重相近。
  - `np.allclose()` 用于检查两组浮点数是否在一定容差范围内相等。

```
def test_run(self):
    # Test correctness of run method
    network = NeuralNetwork(3, 2, 1, 0.5)
    network.weights_input_to_hidden = test_w_i_h.copy()
    network.weights_hidden_to_output = test_w_h_o.copy()

    self.assertTrue(np.allclose(network.run(inputs), 0.09998924))
```

- `test_run`

  ：测试

   

  ```
  run()
  ```

   

  方法输出的正确性。

  - 设置测试权重，运行前向传播，以确保网络层信号传递正确。
  - 输出与预期比较，检查网络运行的精度。

```
suite = unittest.TestLoader().loadTestsFromModule(TestMethods())
unittest.TextTestRunner().run(suite)
```

- 测试执行

  ：创建测试套件并运行测试。

  - `unittest.TestLoader().loadTestsFromModule(TestMethods())` 加载 `TestMethods` 中定义的所有测试。
  - `unittest.TextTestRunner().run(suite)` 在命令行中运行这些测试，并输出测试结果。

### 背景

这组单元测试通过验证 key 组件的准确性来确保神经网络实现的正确性。使用 `unittest` 可以容易地检测和防止代码的回归错误，测试包括网络的初始化、前向传递以及训练过程，以此提供对代码可靠性的重要保障。

___

这段代码用于训练一个神经网络并监控训练过程中的损失值。为了确保模型可以在训练集和验证集上有效地进行预测，我们需要仔细设置和调整超参数，并记录和观测训练和验证结果。下面是逐行解释：

```
import sys
```

- **系统模块 `sys` 的导入**：提供了与Python解释器进行交互的功能，`sys.stdout` 用于将输出写入终端。

```
### Set the hyperparameters here ###
iterations = 3000
learning_rate = 1.0
hidden_nodes = 10
output_nodes = 1
```

- 超参数设置

  ：

  - `iterations`：训练循环的总次数。
  - `learning_rate`：学习率，控制每次权重更新的步幅大小。
  - `hidden_nodes`：隐藏层的节点数。
  - `output_nodes`：输出层的节点数，用于回归任务，这里设置为1。

```
N_i = train_features.shape[1]
print(N_i)
network = NeuralNetwork(N_i, hidden_nodes, output_nodes, learning_rate)
```

- 初始化神经网络

  ：

  - `N_i`：特征数量，即输入层的节点数，由训练数据的特征数量决定。
  - 打印 `N_i` 是用于调试，显示输入节点数是否符合预期。
  - `network`：创建一个 `NeuralNetwork` 实例，使用指定的输入节点、隐藏节点、输出节点和学习率进行初始化。

```
losses = {'train':[], 'validation':[]}
```

- **损失字典初始化**：用来记录训练过程中每次迭代的训练和验证集的损失函数值。

```
for ii in range(iterations):
    # Go through a random batch of 128 records from the training data set
    batch = np.random.choice(train_features.index, size=128)
    X, y = train_features.ix[batch].values, train_targets.ix[batch]['cnt']
```

- 批次训练

  ：

  - 进行 `iterations` 次迭代。在每次迭代中，从训练集中随机抽取128个样本作为小批量数据。
  - `np.random.choice()` 用于从数据索引中随机选择样本索引，确保数据的随机性。
  - `train_features.ix[batch].values` 和 `train_targets.ix[batch]['cnt']` 提取特征值和对应目标`cnt`，作为 `train()` 方法的输入。

```
network.train(X, y)
```

- **训练网络**：调用网络的 `train()` 方法，通过反向传播算法更新网络的权重。

```
# Printing out the training progress
train_loss = MSE(network.run(train_features).T, train_targets['cnt'].values)
val_loss = MSE(network.run(val_features).T, val_targets['cnt'].values)
```

- 计算损失：
  - 在整个训练集和验证集上计算均方误差（MSE），作为损失函数值。
  - 利用 `network.run()` 方法进行前向传播，得到预测输出，并且转置以使维度匹配，随后计算与真实目标的均方误差。

```
    sys.stdout.write("\rProgress: {:2.1f}".format(100 * ii/float(iterations)) \
                     + "% ... Training loss: " + str(train_loss)[:5] \
                     + " ... Validation loss: " + str(val_loss)[:5])
    sys.stdout.flush()
```

- 输出训练进度

  ：

  - 动态更新进度条，显示当前迭代的进度百分比、训练损失和验证损失。
  - `\r` 用来在终端上刷新同一行，`sys.stdout.flush()` 确保立即输出，而不是缓存在缓冲区。

```
    losses['train'].append(train_loss)
    losses['validation'].append(val_loss)
```

- **记录损失**：将每次迭代的训练损失和验证损失分别添加到对应的列表中，以便后续分析和绘图。

### 背景与流程

1. **训练过程**：在每次迭代中，使用一小部分数据更新模型参数，这样有利于提高效率并避免过拟合。
2. **动态监控**：通过记录和输出损失，可以观察模型的收敛情况、调整参数。
3. **超参数调节**：在训练过程中可以调整学习率、隐藏层节点等超参数，以优化模型表现。
4. **验证集**：通过设置验证集，可以评估模型对未见数据的表现，选择最优参数设置。
5. **损失函数**：MSE衡量预测值与真实值之间的偏差，是回归任务中常用的评估指标。

整体来看，这段代码实现了神经网络训练流程的基本框架，适用于小型回归任务的建模与验证。

___

这段代码的目的是将神经网络在测试集上的预测结果与真实数据进行可视化对比。通过绘制图形，我们可以直观地评估模型性能。下面详细解释这段代码：

```
fig, ax = plt.subplots(figsize=(8,4))
```

- **创建图表和轴**：使用 `matplotlib` 创建一个图表和一个子图（轴）。`figsize=(8,4)` 指定图的大小为 8x4 英寸。

```
mean, std = scaled_features['cnt']
predictions = network.run(test_features).T * std + mean
```

- 反标准化预测值

  ：

  - 首先从 `scaled_features` 中获取目标变量 `cnt` 的均值和标准差。
  - `network.run(test_features)` 调用网络的 `run()` 方法以获得预测值。预测输出已经标准化，该步骤反标准化预测结果，使其恢复到原始的数量级。

```
ax.plot(predictions[0], label='Prediction')
ax.plot((test_targets['cnt'] * std + mean).values, label='Data')
```

- 绘制预测和真实数据线

  ：

  - `ax.plot(predictions[0], label='Prediction')` 绘制预测值的曲线。
  - `(test_targets['cnt'] * std + mean).values` 反标准化真实目标值，以同样方式与预测结果在同一幅图中绘制。
  - `label` 参数用于图例中标识两条线对应关系。

```
ax.set_xlim(right=len(predictions))
ax.legend()
```

- 设定图形的 x 轴范围和图例

  ：

  - `ax.set_xlim(right=len(predictions))` 将 x 轴范围设置到预测长度。
  - `ax.legend()` 显示绘图时指定的标签，方便识别预测结果与真实数据。

```
dates = pd.to_datetime(rides.ix[test_data.index]['dteday'])
dates = dates.apply(lambda d: d.strftime('%b %d'))
```

- 日期格式处理

  ：

  - `pd.to_datetime()` 将指定的 `dteday` 列数据转化为日期格式。
  - 使用 `apply(lambda d: d.strftime('%b %d'))` 将日期格式化成 "月 日" 样式，以提升标签可读性。

```
ax.set_xticks(np.arange(len(dates))[12::24])
_ = ax.set_xticklabels(dates[12::24], rotation=45)
```

- 设置 x 轴刻度和标签：
  - `np.arange(len(dates))[12::24]` 确定 x 轴刻度位置，采用每隔一天的第 12 小时的时刻，在图例中标记一个点。
  - `ax.set_xticklabels(dates[12::24], rotation=45)` 设置 x 轴的标签为日期字符串，并旋转 45 度以避免文字重叠。

### 背景及解释

这个可视化过程展现了：

- **模型性能**：直观比较预测结果与实际数据，观察两者的趋势和偏差。
- **反标准化**：在标准化和反标准化过程中，评估某些数值特征时需恢复其原有量纲，以利于解释和分析。
- **时间序列数据绘制**：通过日期标签设置，使得图表更加易读和贴近实际情况。
- **风格设置**：通过调整图形大小、旋转标签、设置图例等，确保可视化结果更易于理解和分析。

通过绘制这种图形，最终了解到我们的神经网络在从未见过的数据上的表现情况，方便我们进行模型调整、选择和优化。